{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%pip install torch torchvision torchaudio\n","%pip install transformers datasets tokenizers seqeval -q\n","%pip install --upgrade pip\n","%pip install transformers[torch]\n","%pip install accelerate -U\n","%pip install pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import datasets\n","import json\n","import csv\n","import numpy as np \n","import pandas as pd\n","from transformers import BertTokenizerFast \n","from transformers import DataCollatorForTokenClassification \n","from transformers import AutoModelForTokenClassification\n","from transformers import TrainingArguments, Trainer\n","from transformers import pipeline\n","from statistics import mode\n","from time import time\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read in data\n","train_raw = pd.read_csv(\"train.csv\", skip_blank_lines=False)\n","validation_raw = pd.read_csv(\"validation.csv\", skip_blank_lines=False)\n","test_raw = pd.read_csv(\"test.csv\", skip_blank_lines=False)\n","train_ready = []\n","validation_ready = []\n","test_ready = []\n","\n","# Format training input for BERT\n","count = 0\n","tokens = []\n","ner_tags = []\n","for _, row in train_raw.iterrows():\n","    if (pd.isna(row['id'])):\n","        train_ready.append(\n","            {\n","                \"id\": id,\n","                \"tokens\": [tokens],\n","                \"ner_tags\": [ner_tags]\n","            }\n","        )\n","        count += 1\n","        tokens = []\n","        ner_tags = []\n","        continue\n","    id = str(count)\n","    tokens.append(row['word'])\n","    ner_tags.append(int(row['label']))\n","train_ready.append(\n","    {\n","        \"id\": id,\n","        \"tokens\": [tokens],\n","        \"ner_tags\": [ner_tags]\n","    }\n",")\n","\n","# Format validation input for BERT\n","count = 0\n","tokens = []\n","ner_tags = []\n","for _, row in validation_raw.iterrows():\n","    if (pd.isna(row['id'])):\n","        validation_ready.append(\n","            {\n","                \"id\": id,\n","                \"tokens\": [tokens],\n","                \"ner_tags\": [ner_tags]\n","            }\n","        )\n","        count += 1\n","        tokens = []\n","        ner_tags = []\n","        continue\n","    id = str(count)\n","    tokens.append(row['word'])\n","    ner_tags.append(int(row['label']))\n","validation_ready.append(\n","    {\n","        \"id\": id,\n","        \"tokens\": [tokens],\n","        \"ner_tags\": [ner_tags]\n","    }\n",")\n","\n","# Format testing input for BERT\n","count = 0\n","tokens = []\n","ner_tags = []\n","for _, row in test_raw.iterrows():\n","    if (pd.isna(row['id'])):\n","        test_ready.append(\n","            {\n","                \"id\": id,\n","                \"tokens\": [tokens]\n","            }\n","        )\n","        count += 1\n","        tokens = []\n","        continue\n","    id = str(count)\n","    tokens.append(row['word'])\n","test_ready.append(\n","    {\n","        \"id\": id,\n","        \"tokens\": [tokens]\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-cased\", ) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Tokenizer function from the tutorial\n","def tokenize_and_align_labels(examples, label_all_tokens=True): \n","    \"\"\"\n","    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n","    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n","\n","    Parameters:\n","    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n","                     - \"tokens\": list of words in a sentence.\n","                     - \"ner_tags\": list of corresponding entity tags for each word.\n","                     \n","    label_all_tokens (bool): A flag to indicate whether all tokens should have labels. \n","                             If False, only the first token of a word will have a label, \n","                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n","\n","    Returns:\n","    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n","    \"\"\"\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n","    labels = [] \n","    for i, label in enumerate(examples[\"ner_tags\"]): \n","        word_ids = tokenized_inputs.word_ids(batch_index=i) \n","        # word_ids() => Return a list mapping the tokens\n","        # to their actual word in the initial sentence.\n","        # It Returns a list indicating the word corresponding to each token. \n","        previous_word_idx = None \n","        label_ids = []\n","        # Special tokens like `<s>` and `<\\s>` are originally mapped to None \n","        # We need to set the label to -100 so they are automatically ignored in the loss function.\n","        for word_idx in word_ids: \n","            if word_idx is None: \n","                # set –100 as the label for these special tokens\n","                label_ids.append(-100)\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            elif word_idx != previous_word_idx:\n","                # if current word_idx is != prev then its the most regular case\n","                # and add the corresponding token                 \n","                label_ids.append(label[word_idx]) \n","            else: \n","                # to take care of sub-words which have the same word_idx\n","                # set -100 as well for them, but only if label_all_tokens == False\n","                label_ids.append(label[word_idx] if label_all_tokens else -100) \n","                # mask the subword representations after the first subword\n","                 \n","            previous_word_idx = word_idx \n","        labels.append(label_ids) \n","    tokenized_inputs[\"labels\"] = labels \n","    return tokenized_inputs "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Tokenize training data\n","tokenized_datasets = []\n","for sent in train_ready:\n","    tokenized_datasets.append(tokenize_and_align_labels(sent))\n","\n","for i, row in enumerate(tokenized_datasets):\n","    tokenized_datasets[i]['input_ids'] = (row['input_ids'])[0]\n","    tokenized_datasets[i]['attention_mask'] = (row['attention_mask'])[0]\n","    tokenized_datasets[i]['token_type_ids'] = (row['token_type_ids'])[0]\n","    tokenized_datasets[i]['labels'] = (row['labels'])[0]\n","\n","# Tokenize validation data\n","tokenized_datasets_validation = []\n","for sent in validation_ready:\n","    tokenized_datasets_validation.append(tokenize_and_align_labels(sent))\n","\n","for i, row in enumerate(tokenized_datasets_validation):\n","    tokenized_datasets_validation[i]['input_ids'] = (row['input_ids'])[0]\n","    tokenized_datasets_validation[i]['attention_mask'] = (row['attention_mask'])[0]\n","    tokenized_datasets_validation[i]['token_type_ids'] = (row['token_type_ids'])[0]\n","    tokenized_datasets_validation[i]['labels'] = (row['labels'])[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\"bert-large-cased\", num_labels=21)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["args = TrainingArguments( \n","\"test-ner\",\n","evaluation_strategy = \"epoch\", \n","learning_rate=2e-5, \n","per_device_train_batch_size=16, \n","per_device_eval_batch_size=16, \n","num_train_epochs=20, \n","weight_decay=0.01, \n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metric = datasets.load_metric(\"seqeval\") "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_list = ['B-company', 'B-facility', 'B-geo-loc', 'B-movie', 'B-musicartist', 'B-other', 'B-person', 'B-product', 'B-sportsteam', 'B-tvshow', 'I-company', 'I-facility', 'I-geo-loc', 'I-movie', 'I-musicartist', 'I-other', 'I-person', 'I-product', 'I-sportsteam', 'I-tvshow', 'O']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Metrics computing function from the tutorial\n","def compute_metrics(eval_preds): \n","    \"\"\"\n","    Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n","    The function computes precision, recall, F1 score and accuracy.\n","\n","    Parameters:\n","    eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n","\n","    Returns:\n","    A dictionary containing the precision, recall, F1 score and accuracy.\n","    \"\"\"\n","    pred_logits, labels = eval_preds \n","    \n","    pred_logits = np.argmax(pred_logits, axis=2) \n","    # the logits and the probabilities are in the same order,\n","    # so we don’t need to apply the softmax\n","    \n","    # We remove all the values where the label is -100\n","    predictions = [ \n","        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n","        for prediction, label in zip(pred_logits, labels) \n","    ] \n","    \n","    true_labels = [ \n","      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n","       for prediction, label in zip(pred_logits, labels) \n","   ] \n","    results = metric.compute(predictions=predictions, references=true_labels) \n","    return { \n","   \"precision\": results[\"overall_precision\"], \n","   \"recall\": results[\"overall_recall\"], \n","   \"f1\": results[\"overall_f1\"], \n","  \"accuracy\": results[\"overall_accuracy\"], \n","  } "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Trainer( \n","    model, \n","    args, \n","   train_dataset=tokenized_datasets, \n","   eval_dataset=tokenized_datasets_validation, \n","   data_collator=data_collator, \n","   tokenizer=tokenizer, \n","   compute_metrics=compute_metrics \n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start_train = time()\n","trainer.train()\n","end_train = time()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training time\n","print((end_train - start_train)/60, \"mins\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer.save_pretrained(\"tokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["id2label = {\n","    str(i): label for i,label in enumerate(label_list)\n","}\n","label2id = {\n","    label: str(i) for i,label in enumerate(label_list)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = json.load(open(\"ner_model/config.json\"))\n","config[\"id2label\"] = id2label\n","config[\"label2id\"] = label2id\n","json.dump(config, open(\"ner_model/config.json\",\"w\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n","\n","tag_words = {}\n","out = open('ans.csv', 'w')\n","submit = csv.writer(out)\n","submit.writerow(['id', 'label'])\n","global_id = 0\n","\n","start_test = time()\n","for row in tqdm(test_ready):\n","    # Join every test sentence into a string\n","    sent = \" \".join(row['tokens'][0])\n","    intervals = []\n","    containers = []\n","    track = 0\n","    \n","    # Determine intervals spanned by each word\n","    for token in row['tokens'][0]:\n","        intervals.append((track, track + len(token)))\n","        containers.append([])\n","        track += len(token) + 1\n","    # Feed a sentence into the model for prediction\n","    ner_results = nlp(sent)\n","    # Map all predicted subwords with corresponding full words\n","    for index, word in enumerate(ner_results):\n","        for pos, interval in enumerate(intervals):\n","            if word['start'] >= interval[0] and word['end'] <= interval[1]:\n","                containers[pos].append(word['entity'])\n","    # Assign tags to words based on the majority of corresponding subword tags\n","    for j, container in enumerate(containers):\n","        if len(container) > 0:\n","            containers[j] = mode(container)\n","        else:\n","            containers[j] = 'O'\n","    \n","    # Form the output\n","    for word in containers:\n","        submit.writerow([global_id, label_list.index(word)])\n","        global_id += 1\n","out.close()\n","end_test = time()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Testing time\n","print((end_test - start_test)/60, \"mins\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
